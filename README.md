**SheetCopilot**: Bringing Software Productivity to the Next Level through Large Language Models
========

<p align="center">
<img src="assets/icon.png" width="50%">
</p>

<p align="center">
<br />
<a href="https://sheetcopilot-demo.github.io/"><strong>Explore the project website »</strong></a>
<br />
</p>

We release the SheetCopilot as well as the evaluation environment in this repository.

SheetCopilot is an assistant agent that manipulate spreadsheets by following user commands. It breaks new ground in human-computer interaction, opening up possibilities for enabling non-expert users to complete their mandane work on complex software (e.g. Google sheets and Excel) via a language interface.

# How SheetCopilot Works

SheetCopilot employs a novel way of directing Large Language Models (LLMs) to manipulate spreadsheets like a human expert. To achieve elegant closed-loop control, SheetCopilot observes the spreadsheet state and polishes generated solutions according to external action documents and error feedback, thereby improving its success rate and efficiency.

<p align="center">
<img src="assets/SheetCopilot-teaser.png" width="5100%">
</p>
<br>

# Setup
## 1. Prepare the Conda environment

Python 3.10 is required to support the asyncronous implementation of SheetCopilot.

```
conda create -n habitat python=3.10
```

## 2. In this conda env, run:

```
pip install -r requirements.txt
```


# Dataset
We release a spreadsheet task dataset containing 28 workbooks and 221 tasks applied to these workbooks. Each task is given one or more hand-made solutions.

This dataset can be used to evaluate any spreadsheet manipulation agent including RL, LLM-based or rule-based methods.

In the ```dataset``` folder, ```dataset.xlsx``` lists the 221 tasks, containing the target workbook name, task number, instruction, task categories, and involved atomic actions.

The ```task_sheets``` folder contains the 28 evaluation workbooks these tasks applied to.

The ```task_sheet_answers``` folder contains the reference solutions of the tasks. Each solution consists of a reference workbook showing the expected outcome of the corresponding task instruction and a *.yaml file listing the necessary sheet states to compare. If the necessary states of the result matches any of the references, the result is seen as correct.

The ```dataset_20Samples.xlsx``` file lists the 20 selected tasks used to compare the LLMs in our experiments.


# Evaluation
The results generated by any method should be organized like this:

```
results_dir
  └── ([NO.]_[Sheet Name])
  └── 1_BoomerangSales
  |   └── ([NO.]_[Sheet Name]_[Repeat_NO.].xlsx)
  |   └── 1_BoomerangSales_1.xlsx
  |   ...
  |   └── 1_BoomerangSales_3.xlsx
  ...
  └── 9_BoomerangSales
  ...
  └── 1_Dragging
  ...
  └── 8_Dragging
  ...
```

[Sheet Name] and [NO.] are columns A and B in ```dataset.xlsx```. [Repeat_NO.] is used to differentiate multiple repeats of the same task. If you run each task only once, [Repeat_NO.] is 1.

Run this code within ```eval``` to evaluate your results:
```
python .\evaluation.py -d [result dir] -r [number of repeats]
```

The evaluation results will be recorded in a file named ```eval_result.yaml``` under the result folder.

The evaluation can restart from a checkpoint if it has been aborted.


# SheetCopilot Usage

Firt of all, an OpenAI API key is required.

## For Excel
Coming soon...

## For Google Sheets

## Which aspects of a spreadsheet can SheetCopilot control


# Citation
SheetCopilot and the dataset can only be used for non-conmercial purposes.

If you use the SheetCopilot framework or data, feel free to cite us.

```bibtex
@misc{li2023sheetcopilot,
  title={SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models},
  author={Hongxin Li and Jingran Su and Yuntao Chen and Qing Li and Zhaoxiang Zhang},
  journal={arXiv preprint arXiv:2305.19308},
  year={2023}
}