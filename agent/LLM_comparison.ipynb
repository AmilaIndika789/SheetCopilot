{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pathlib\n",
    "import yaml\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../benchmark_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_generator(file_path):\n",
    "    return pathlib.Path(f\"{file_path}\").glob(\"**/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path, content_type):\n",
    "    with open(file_path) as file:\n",
    "        instructions = yaml.load(file, Loader=yaml.Loader)\n",
    "    return yaml.dump(instructions[content_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes_and_new_lines(instructions: str) -> list:\n",
    "    instructions = instructions.replace(\"'\", \"\").replace('\"', \"\").split(\"- Step\")\n",
    "    instructions = [\n",
    "        instruction.strip().replace(\"\\n\", \"\")\n",
    "        for instruction in instructions\n",
    "        if instruction != \"\"\n",
    "    ]\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generator = create_path_generator(\n",
    "    file_path=f\"{OUTPUT_PATH}/intermediate_responses/\"\n",
    ")\n",
    "correct_response_file_paths = [str(path) for path in path_generator]\n",
    "correct_response_file_paths = correct_response_file_paths[4:111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\Amilas_Windows_VM\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bleu\\9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Thu Aug 29 09:22:20 2024) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\Amilas_Windows_VM\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--google_bleu\\6fc70b7be0088120a372dfdd5d320b39b8bb3630cb8029b193941d9376e86bb0 (last modified on Thu Aug 29 09:31:05 2024) since it couldn't be found locally at evaluate-metric--google_bleu, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\Amilas_Windows_VM\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Thu Aug 29 09:34:34 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\Amilas_Windows_VM\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--meteor\\e7ed321a1b44c34fa4679192809db2cee7e3bd4bba0fe8b76061d807706c2374 (last modified on Sun Sep 15 16:49:31 2024) since it couldn't be found locally at evaluate-metric--meteor, or remotely on the Hugging Face Hub.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "EXAMPLES_COUNT = 107\n",
    "llm_metrics = {}\n",
    "llm_models = [\n",
    "    \"gemma2-9b-it\",\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"mixtral-8x7b-32768\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "]\n",
    "for model_name in tqdm(llm_models):\n",
    "    bleu_scores = []\n",
    "    gleu_scores = []\n",
    "    rouge_scores = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    if model_name == \"mixtral-8x7b-32768\":\n",
    "        few_shot_count = 1  # Used a smaller few shot due to smaller context window\n",
    "    else:\n",
    "        few_shot_count = 4\n",
    "\n",
    "    path_generator = create_path_generator(\n",
    "        file_path=f\"{OUTPUT_PATH}/{model_name}/model_responses/{few_shot_count}_shot/\"\n",
    "    )\n",
    "    llm_responses_file_paths = [str(path) for path in path_generator]\n",
    "    for test_index in tqdm(range(EXAMPLES_COUNT)):\n",
    "        correct_file_path = correct_response_file_paths[test_index]\n",
    "        predicted_file_path = llm_responses_file_paths[test_index]\n",
    "\n",
    "        # Correct instructions\n",
    "        correct_instructions = read_yaml_file(\n",
    "            file_path=correct_file_path, content_type=\"intermediate response\"\n",
    "        )\n",
    "        correct_instructions = remove_quotes_and_new_lines(correct_instructions)\n",
    "\n",
    "        # predicted instructions\n",
    "        predicted_instructions = read_yaml_file(\n",
    "            file_path=predicted_file_path, content_type=f\"{model_name}_response\"\n",
    "        )\n",
    "        predicted_instructions = remove_quotes_and_new_lines(predicted_instructions)\n",
    "        predicted_instructions = [\"\\n\".join(predicted_instructions)]\n",
    "        correct_instructions = [\"\\n\".join(correct_instructions)]\n",
    "\n",
    "        if len(predicted_instructions) != len(correct_instructions):\n",
    "            print(f\"Predicted file: {predicted_file_path}\")\n",
    "            print(f\"Correct file: {correct_file_path}\")\n",
    "            print(f\"Predicted: {predicted_instructions}\")\n",
    "            print(f\"Correct: {correct_instructions}\")\n",
    "\n",
    "        # BLEU\n",
    "        bleu_results = bleu.compute(\n",
    "            predictions=predicted_instructions, references=correct_instructions\n",
    "        )\n",
    "        bleu_scores.append(bleu_results[\"bleu\"])\n",
    "\n",
    "        # GLEU\n",
    "        google_bleu_results = google_bleu.compute(\n",
    "            predictions=predicted_instructions, references=correct_instructions\n",
    "        )\n",
    "        gleu_scores.append(google_bleu_results[\"google_bleu\"])\n",
    "\n",
    "        # Rouge-L\n",
    "        rouge_results = rouge.compute(\n",
    "            predictions=predicted_instructions, references=correct_instructions\n",
    "        )\n",
    "        rouge_scores.append(rouge_results[\"rougeL\"])\n",
    "\n",
    "        # METEOR\n",
    "        meteor_results = meteor.compute(\n",
    "            predictions=predicted_instructions, references=correct_instructions\n",
    "        )\n",
    "        meteor_scores.append(meteor_results[\"meteor\"])\n",
    "\n",
    "        Z_90_CONFIDENCE_INTERVAL = 1.645\n",
    "        metrics = {\n",
    "            \"mean_bleu\": np.mean(bleu_scores),\n",
    "            \"std_bleu\": np.std(bleu_scores),\n",
    "            \"ci_bleu\": (Z_90_CONFIDENCE_INTERVAL * np.std(bleu_scores))\n",
    "            / np.sqrt(EXAMPLES_COUNT),\n",
    "            \"mean_gleu\": np.mean(gleu_scores),\n",
    "            \"std_gleu\": np.std(gleu_scores),\n",
    "            \"ci_gleu\": (Z_90_CONFIDENCE_INTERVAL * np.std(gleu_scores))\n",
    "            / np.sqrt(EXAMPLES_COUNT),\n",
    "            \"mean_rouge\": np.mean(rouge_scores),\n",
    "            \"std_rouge\": np.std(rouge_scores),\n",
    "            \"ci_rouge\": (Z_90_CONFIDENCE_INTERVAL * np.std(rouge_scores))\n",
    "            / np.sqrt(EXAMPLES_COUNT),\n",
    "            \"mean_meteor\": np.mean(meteor_scores),\n",
    "            \"std_meteor\": np.std(meteor_scores),\n",
    "            \"ci_meteor\": (Z_90_CONFIDENCE_INTERVAL * np.std(meteor_scores))\n",
    "            / np.sqrt(EXAMPLES_COUNT),\n",
    "        }\n",
    "        llm_metrics[model_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to plot\n",
    "metrics = [\"bleu\", \"gleu\", \"rouge\", \"meteor\"]\n",
    "colors = [\"b\", \"g\", \"r\", \"m\", \"y\"]\n",
    "models = list(llm_metrics.keys())\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract values for each model\n",
    "    means = [llm_metrics[model][f\"mean_{metric}\"] for model in models]\n",
    "    stds = [llm_metrics[model][f\"std_{metric}\"] for model in models]\n",
    "    cis = [llm_metrics[model][f\"ci_{metric}\"] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))  # X-axis positions\n",
    "    lower_bounds = np.array(means) - np.array(cis)\n",
    "    upper_bounds = np.array(means) + np.array(cis)\n",
    "    \n",
    "    # Plot mean values\n",
    "    ax.bar(x, means, yerr=cis, color=colors, alpha=0.6, capsize=5)\n",
    "    \n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "    ax.set_title(f\"Mean {metric.upper()} Scores by Model\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid()\n",
    "    # ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"LLM_comparison.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
