{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import yaml\n",
    "import nltk\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "import pathlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_generator(file_path):\n",
    "    return pathlib.Path(f\"{file_path}\").glob(\"**/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path, content_type):\n",
    "    with open(file_path) as file:\n",
    "        instructions = yaml.load(file, Loader=yaml.Loader)\n",
    "    return yaml.dump(instructions[content_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generator = create_path_generator(\n",
    "    file_path=f\"{OUTPUT_PATH}/intermediate_responses/\"\n",
    ")\n",
    "correct_response_file_paths = [str(path) for path in path_generator]\n",
    "correct_response_file_paths = correct_response_file_paths[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generator = create_path_generator(file_path=f\"{OUTPUT_PATH}/gpt_responses/\")\n",
    "gpt_responses_file_paths = [str(path) for path in path_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4af473035d44521a5d70d42cd1b56fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Amilas_Windows_VM\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "google_bleu = evaluate.load(\"google_bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted file: ..\\output_dir\\gpt_responses\\2_Dragging_gpt_response.yaml\n",
      "Correct file: ..\\output_dir\\intermediate_responses\\2_Dragging.yaml\n",
      "Predicted file: ..\\output_dir\\gpt_responses\\2_FutureValue_gpt_response.yaml\n",
      "Correct file: ..\\output_dir\\intermediate_responses\\2_FutureValue.yaml\n",
      "Predicted file: ..\\output_dir\\gpt_responses\\3_Dragging_gpt_response.yaml\n",
      "Correct file: ..\\output_dir\\intermediate_responses\\3_Dragging.yaml\n",
      "Predicted: ['- \\'Step 1: Fill down column B from cell B2 to B122 using the formula in B2.\\'- \\'Step 2: Create a new sheet named \"ScatterChart\".\\'- \\'Step 3: Generate a scatter chart in \"ScatterChart\" based on data from columns A  and B in \"Sheet1\".\\'- \\'Step 4: Set the chart title to \"Acceleration vs Hanging Mass\".\\'- \\'Step 5: Label the X-axis as \"Hanging Mass (m2) (kg)\".\\'- \\'Step 6: Label the Y-axis as \"Acceleration (m/s^2)\".\\'']\n",
      "Correct: ['1. Fill out the rest of the rows in column B using the formula in B2.', '2. Create a new sheet for the scatter chart.', '3. Create a scatter chart in the new sheet with acceleration on the y-axis  and the hanging mass on the x-axis.', '4. Set the title for the scatter chart.', '5. Set the x-axis title for the scatter chart.', '6. Set the y-axis title for the scatter chart.']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (1) and references (6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# BLEU\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m bleu_results \u001b[38;5;241m=\u001b[39m \u001b[43mbleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_instructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrect_instructions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m bleu_scores\u001b[38;5;241m.\u001b[39mappend(bleu_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# GLEU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amilas_Windows_VM\\anaconda3\\envs\\llm_ss\\lib\\site-packages\\evaluate\\module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amilas_Windows_VM\\anaconda3\\envs\\llm_ss\\lib\\site-packages\\evaluate\\module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatch in the number of predictions (1) and references (6)"
     ]
    }
   ],
   "source": [
    "bleu_scores = []\n",
    "gleu_scores = []\n",
    "rouge_scores = []\n",
    "meteor_scores = []\n",
    "for test_index in range(len(correct_response_file_paths)):\n",
    "    correct_file_path = correct_response_file_paths[test_index]\n",
    "    predicted_file_path = gpt_responses_file_paths[test_index]\n",
    "\n",
    "    # Correct instructions\n",
    "    correct_instructions = read_yaml_file(\n",
    "        file_path=correct_file_path, content_type=\"intermediate response\"\n",
    "    ).astype(str).split(\"- Step\")\n",
    "    correct_instructions = [\n",
    "        instruction.strip().replace(\"\\n\", \"\")\n",
    "        for instruction in correct_instructions\n",
    "        if instruction != \"\"\n",
    "    ]\n",
    "\n",
    "    # predicted instructions\n",
    "    predicted_instructions = read_yaml_file(\n",
    "        file_path=predicted_file_path, content_type=\"gpt_response\"\n",
    "    ).astype(str).split(\"- Step\")\n",
    "    predicted_instructions = [\n",
    "        instruction.strip().replace(\"\\n\", \"\")\n",
    "        for instruction in predicted_instructions\n",
    "        if instruction != \"\"\n",
    "    ]\n",
    "    print(f\"Predicted file: {predicted_file_path}\")\n",
    "    print(f\"Correct file: {correct_file_path}\")\n",
    "    if len(predicted_instructions) != len(correct_instructions):\n",
    "        print(f\"Predicted: {predicted_instructions}\")\n",
    "        print(f\"Correct: {correct_instructions}\")\n",
    "\n",
    "    # BLEU\n",
    "    bleu_results = bleu.compute(predictions=predicted_instructions, references=correct_instructions)\n",
    "    bleu_scores.append(bleu_results['bleu'])\n",
    "\n",
    "    # GLEU\n",
    "    google_bleu_results = google_bleu.compute(predictions=predicted_instructions, references=correct_instructions)\n",
    "    gleu_scores.append(google_bleu_results['google_bleu'])\n",
    "\n",
    "    # Rouge-L\n",
    "    rouge_results = rouge.compute(predictions=predicted_instructions, references=correct_instructions)\n",
    "    rouge_scores.append(rouge_results['rougeL'])\n",
    "\n",
    "    # METEOR\n",
    "    meteor_results = meteor.compute(predictions=predicted_instructions, references=correct_instructions)\n",
    "    meteor_results['meteor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BLEU: {np.mean(bleu_scores):.3f} {u'\\u00B1'} {np.std(bleu_scores):.3f}\")\n",
    "print(f\"GLEU: {np.mean(gleu_scores):.3f} {u'\\u00B1'} {np.std(gleu_scores):.3f}\")\n",
    "print(f\"ROUGE-L: {np.mean(rouge_scores):.3f} {u'\\u00B1'} {np.std(rouge_scores):.3f}\")\n",
    "print(f\"METEOR: {np.mean(meteor_scores):.3f} {u'\\u00B1'} {np.std(meteor_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
