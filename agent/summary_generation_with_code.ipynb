{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompt import Prompt\n",
    "from utils.ChatGPT import ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"./config/config.yaml\"\n",
    "OUTPUT_PATH = \"../benchmark_dir\"\n",
    "AGENT_NAME = \"ChatGPT_1\" # Available: ChatGPT_1 (4o-mini), ChatGPT_2 (4o), Llama_3_1, Gemma_2, Mistral\n",
    "MODEL_NAME = \"gpt-4o-mini\" # Available: gpt-4o, gpt-4o-mini, llama-3.1-70b-versatile, gemma2-9b-it, mixtral-8x7b-32768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_agent_configuration(configuration_file_path, agent_name):\n",
    "    with open(configuration_file_path, mode=\"r\") as file:\n",
    "        config = yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "    agent_config = config[\"Agent\"]\n",
    "    # GPT-4o-mini\n",
    "    if agent_name == \"ChatGPT_1\": # gpt-4o-mini\n",
    "        agent_config[\"ChatGPT_1\"][\"api_keys\"] = [os.environ[\"OPENAI_API_KEY\"]]\n",
    "    elif agent_name == \"ChatGPT_2\": # gpt-4o\n",
    "        agent_config[\"ChatGPT_2\"][\"api_keys\"] = [os.environ[\"OPENAI_API_KEY\"]]\n",
    "    else:\n",
    "        agent_config[\"Llaama_3_1\"][\"api_keys\"] = [os.environ[\"GROQ_API_KEY\"]]\n",
    "        agent_config[\"Gemma_2\"][\"api_keys\"] = [os.environ[\"GROQ_API_KEY\"]]\n",
    "        agent_config[\"Mistral\"][\"api_keys\"] = [os.environ[\"GROQ_API_KEY\"]]\n",
    "    return agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_if_non_existing(path):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt(prompt, test_input_file_path, agent_config, few_shot_count):\n",
    "    prompt_filename = test_input_file_path.split('\\\\')[-1].split('.')[0] + '_prompt.txt'\n",
    "    agent_name = agent_config[\"ChatGPT_1\"][\"model_name\"]\n",
    "    create_path_if_non_existing(f\"{OUTPUT_PATH}/{agent_name}/prompts/{few_shot_count}_shot\")\n",
    "    with open(f\"{OUTPUT_PATH}/{agent_name}/prompts/{few_shot_count}_shot/{prompt_filename}\", \"w\") as file:\n",
    "        file.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gpt_response(predicted_instructions, test_input_file_path, agent_config, few_shot_count):\n",
    "    gpt_response_filename = test_input_file_path.split('\\\\')[-1].split('.')[0] + \"_gpt_response.yaml\"\n",
    "    gpt_response = {\"gpt_response\": predicted_instructions}\n",
    "    agent_name = agent_config[\"ChatGPT_1\"][\"model_name\"]\n",
    "    create_path_if_non_existing(f\"{OUTPUT_PATH}/{agent_name}/gpt_responses/{few_shot_count}_shot\")\n",
    "    with open(f\"{OUTPUT_PATH}/{agent_name}/gpt_responses/{few_shot_count}_shot/{gpt_response_filename}\", \"w\") as file:\n",
    "        yaml.dump(gpt_response, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = set_agent_configuration(configuration_file_path=CONFIG_FILE, agent_name=AGENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Few-shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\n",
    "    agent_configuration=agent_config, agent_name=AGENT_NAME, model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = prompt.create_few_shot_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_prompt = prompt.create_actual_prompt(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set (except one-shot example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_chat_gpt(prompt):\n",
    "    try:\n",
    "        chatbot = ChatGPT(agent_config[\"ChatGPT_1\"], context=[], interaction_mode=True)\n",
    "        response = await chatbot(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"error occurs when parsing response: {e}\")\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_index in tqdm(range(11, 21)):\n",
    "    test_input_file_path = input_file_paths[test_index]\n",
    "    test_src_file_path = excel_file_paths[test_index]\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = (\n",
    "        \"SYSTEM\\n\"\n",
    "        \"Summarize the each sub-step of instructions into explanations in natural language. \"\n",
    "        \"Be brief and do not provide verbose explanations.\"\n",
    "        \"Avoid redundant steps and provide minimal steps\\n\\n\"\n",
    "        \"{few_shot_examples}\\n\"\n",
    "        \"USER\\n\"\n",
    "        \"{actual_input}\\n\"\n",
    "        \"Here is the supplementary documentation you can reference:\\n\"\n",
    "        \"{actual_documentation}\\n\"\n",
    "        \"Here is the corresponding sheet state:\\n\"\n",
    "        \"{actual_sheet_state}\\n\"\n",
    "    )\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = prompt.format(\n",
    "        few_shot_examples = \"\\n\".join(few_shot_examples),\n",
    "        actual_input=get_input_functions(test_input_file_path),\n",
    "        actual_documentation=extract_docs_for_input_functions(\n",
    "            input_file_path=test_input_file_path,\n",
    "            agent_config=agent_config,\n",
    "            api_doc=api_doc,\n",
    "        ),\n",
    "        actual_sheet_state=get_sheet_state(\n",
    "            file_path=test_src_file_path, backend=xw_backend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    save_prompt(prompt, test_input_file_path, agent_config, no_of_examples)\n",
    "\n",
    "    # Get GPT response\n",
    "    response = await call_chat_gpt(prompt)\n",
    "\n",
    "    predicted_instructions = response.split(\"\\n\")\n",
    "    predicted_instructions = [instruction[2:] for instruction in predicted_instructions]\n",
    "    save_gpt_response(predicted_instructions, test_input_file_path, agent_config, no_of_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
