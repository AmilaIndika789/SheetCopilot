{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from Agent.xwAPI import xwBackend\n",
    "from utils.construct_prompt import get_api_doc\n",
    "from utils.ChatGPT import ChatGPT\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"./config/config.yaml\"\n",
    "OUTPUT_PATH = \"../output_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_agent_configuration(configuration_file_path):\n",
    "    with open(configuration_file_path, mode=\"r\") as file:\n",
    "        config = yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "    agent_config = config[\"Agent\"]\n",
    "    agent_config[\"ChatGPT_1\"][\"api_keys\"] = [os.environ[\"OPENAI_API_KEY\"]]\n",
    "    return agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_documentation(agent_config):\n",
    "    with open(agent_config[\"api_doc_path\"]) as f:\n",
    "        api_doc = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return api_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excel_backend(agent_config, api_doc):\n",
    "    if agent_config[\"API_backend\"] == \"xw\":\n",
    "        xw_backend = xwBackend(agent_config[\"APP_backend\"], api_doc)\n",
    "    return xw_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_generator(file_path):\n",
    "    return pathlib.Path(f\"{file_path}\").glob(\"**/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_excel_filenames(path_generator):\n",
    "    response_log_paths = [\n",
    "        str(path).split(\"\\\\\")[-1] for path in path_generator if path.is_file()\n",
    "    ]\n",
    "    return [f\"{log_file.split('_')[-1][:-5]}.xlsx\" for log_file in response_log_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sheet_state(file_path, backend):\n",
    "    if file_path is not None:\n",
    "        time.sleep(0.5)\n",
    "        backend.OpenWorkbook(file_path)\n",
    "\n",
    "    return backend.GetSheetsState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docs_for_input_functions(input_file_path, agent_config, api_doc):\n",
    "    with open(input_file_path) as file:\n",
    "        input_functions = yaml.load(file, Loader=yaml.Loader)\n",
    "    prompt_format = agent_config[\"ChatGPT_1\"][\"prompt_format\"]\n",
    "    api_list, api_usage, api_detail_doc = get_api_doc(prompt_format, api_doc)\n",
    "    # Filter upto first open paranthesis\n",
    "    unique_function_names = set(\n",
    "        [\n",
    "            function[: function.find(\"(\")]\n",
    "            for function in input_functions[\"refined_response\"]\n",
    "        ]\n",
    "    )\n",
    "    documentation_for_functions = [\n",
    "        api_detail_doc[name] for name in unique_function_names\n",
    "    ]\n",
    "    return documentation_for_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_functions(input_file_path):\n",
    "    with open(input_file_path) as file:\n",
    "        input_functions = yaml.load(file, Loader=yaml.Loader)\n",
    "    return yaml.dump(input_functions[\"refined_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_summarization(correct_file_path):\n",
    "    with open(correct_file_path) as file:\n",
    "        correct_summarizations = yaml.load(file, Loader=yaml.Loader)\n",
    "    return yaml.dump(correct_summarizations[\"intermediate response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_if_non_existing(path):\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt(prompt, test_input_file_path, agent_config, few_shot_count):\n",
    "    prompt_filename = test_input_file_path.split('\\\\')[-1].split('.')[0] + '_prompt.txt'\n",
    "    agent_name = agent_config[\"ChatGPT_1\"][\"model_name\"]\n",
    "    create_path_if_non_existing(f\"{OUTPUT_PATH}/{agent_name}/prompts/{few_shot_count}_shot\")\n",
    "    with open(f\"{OUTPUT_PATH}/{agent_name}/prompts/{few_shot_count}_shot/{prompt_filename}\", \"w\") as file:\n",
    "        file.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gpt_response(predicted_instructions, test_input_file_path, agent_config, few_shot_count):\n",
    "    gpt_response_filename = test_input_file_path.split('\\\\')[-1].split('.')[0] + \"_gpt_response.yaml\"\n",
    "    gpt_response = {\"gpt_response\": predicted_instructions}\n",
    "    agent_name = agent_config[\"ChatGPT_1\"][\"model_name\"]\n",
    "    create_path_if_non_existing(f\"{OUTPUT_PATH}/{agent_name}/gpt_responses/{few_shot_count}_shot\")\n",
    "    with open(f\"{OUTPUT_PATH}/{agent_name}/gpt_responses/{few_shot_count}_shot/{gpt_response_filename}\", \"w\") as file:\n",
    "        yaml.dump(gpt_response, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = set_agent_configuration(configuration_file_path=CONFIG_FILE)\n",
    "api_doc = get_api_documentation(agent_config=agent_config)\n",
    "xw_backend = get_excel_backend(agent_config=agent_config, api_doc=api_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generator = create_path_generator(file_path=f\"{OUTPUT_PATH}/refined_responses/\")\n",
    "input_file_paths = [str(path) for path in path_generator]\n",
    "# input_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generator = create_path_generator(file_path=f\"{OUTPUT_PATH}/refined_responses/\")\n",
    "excel_file_names = get_source_excel_filenames(path_generator=path_generator)\n",
    "excel_file_paths = [\n",
    "    f\"../dataset/task_sheets/{file_name}\" for file_name in excel_file_names\n",
    "]\n",
    "# excel_file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Few-shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = []\n",
    "for example_index in range(no_of_examples):\n",
    "    example_src_file_path = excel_file_paths[example_index]\n",
    "    example_input_file_path = input_file_paths[example_index]\n",
    "\n",
    "    path_generator = create_path_generator(\n",
    "        file_path=f\"{OUTPUT_PATH}/intermediate_responses/\"\n",
    "    )\n",
    "    correct_response_file_paths = [str(path) for path in path_generator]\n",
    "\n",
    "    example_correct_file_path = correct_response_file_paths[example_index]\n",
    "\n",
    "    example_sheet_state = get_sheet_state(\n",
    "        file_path=example_src_file_path, backend=xw_backend\n",
    "    )\n",
    "\n",
    "    example = (\n",
    "        \"USER\\n\"\n",
    "        \"{input_example}\\n\"\n",
    "        \"Here is the supplementary documentation you can reference:\\n\"\n",
    "        \"{documentation_example}\\n\"\n",
    "        \"Here is the corresponding sheet state:\\n\"\n",
    "        \"{sheet_state_example}\\n\\n\"\n",
    "        \"ASSISTANT\\n\"\n",
    "        \"{correct_example}\\n\"\n",
    "    )\n",
    "\n",
    "    example = example.format(\n",
    "        input_example=get_input_functions(example_input_file_path),\n",
    "        documentation_example=extract_docs_for_input_functions(\n",
    "            input_file_path=example_input_file_path,\n",
    "            agent_config=agent_config,\n",
    "            api_doc=api_doc,\n",
    "        ),\n",
    "        correct_example=get_correct_summarization(\n",
    "            correct_file_path=example_correct_file_path\n",
    "        ),\n",
    "        sheet_state_example=get_sheet_state(\n",
    "            file_path=example_src_file_path, backend=xw_backend\n",
    "        ),\n",
    "    )\n",
    "    few_shot_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set (except one-shot example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_chat_gpt(prompt):\n",
    "    try:\n",
    "        chatbot = ChatGPT(agent_config[\"ChatGPT_1\"], context=[], interaction_mode=True)\n",
    "        response = await chatbot(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"error occurs when parsing response: {e}\")\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  3.92s/it]\n"
     ]
    }
   ],
   "source": [
    "for test_index in tqdm(range(11, 21)):\n",
    "    test_input_file_path = input_file_paths[test_index]\n",
    "    test_src_file_path = excel_file_paths[test_index]\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = (\n",
    "        \"SYSTEM\\n\"\n",
    "        \"Summarize the each sub-step of instructions into explanations in natural language. \"\n",
    "        \"Be brief and do not provide verbose explanations.\"\n",
    "        \"Avoid redundant steps and provide minimal steps\\n\\n\"\n",
    "        \"{few_shot_examples}\\n\"\n",
    "        \"USER\\n\"\n",
    "        \"{actual_input}\\n\"\n",
    "        \"Here is the supplementary documentation you can reference:\\n\"\n",
    "        \"{actual_documentation}\\n\"\n",
    "        \"Here is the corresponding sheet state:\\n\"\n",
    "        \"{actual_sheet_state}\\n\"\n",
    "    )\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = prompt.format(\n",
    "        few_shot_examples = \"\\n\".join(few_shot_examples),\n",
    "        actual_input=get_input_functions(test_input_file_path),\n",
    "        actual_documentation=extract_docs_for_input_functions(\n",
    "            input_file_path=test_input_file_path,\n",
    "            agent_config=agent_config,\n",
    "            api_doc=api_doc,\n",
    "        ),\n",
    "        actual_sheet_state=get_sheet_state(\n",
    "            file_path=test_src_file_path, backend=xw_backend\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    save_prompt(prompt, test_input_file_path, agent_config, no_of_examples)\n",
    "\n",
    "    # Get GPT response\n",
    "    response = await call_chat_gpt(prompt)\n",
    "\n",
    "    predicted_instructions = response.split(\"\\n\")\n",
    "    predicted_instructions = [instruction[2:] for instruction in predicted_instructions]\n",
    "    save_gpt_response(predicted_instructions, test_input_file_path, agent_config, no_of_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
